{
    "contents" : "#Pick random points inside shapefile polygon\n#Use Google distance matrix API to fetch network distance/time and addresses\n#https://developers.google.com/maps/documentation/distancematrix/\n#Store and save results as CSV\n\nlibrary(rgdal)\nlibrary(rgeos)\nlibrary(httr)\nlibrary(jsonlite)\n\n#Load properties file\n#Nabbed from http://stackoverflow.com/questions/13681310/reading-configuration-from-text-file\n# myProp <- read.table(\"config.txt\", header=FALSE, sep=\"=\", row.names=1, strip.white=TRUE, na.strings=\"NA\", stringsAsFactors=FALSE)\n# myProp <- setNames(myProp[,1],row.names(myProp))\n# myProp[\"dateOfLastLimitBreach\"]\n\n#record of failed calls\nfail = 0\n\n#Base URL, will knock query together below\ntestURL <- \"http://maps.googleapis.com/maps/api/distancematrix/json\"\n\n#Use single shapefile of Great Britain\ngbmerge <- readOGR(dsn=\"GB_merged\", \"GB_merged\")\n\n#Store the points\n#Number of origin-destination pairs to create and fetch\n#Note there's a pause of at least 0.2 seconds between each\n#so getting >2000 of your daily google API allowance\n#Might be around 7 minutes\n#The csv won't be saved until the end of the process\n#Job: add option to save regularly so's not to risk wasting API call limit\n#DAILY ALLOWANCE IS 2500.\npairNum = 2000\n\n#http://casoilresource.lawr.ucdavis.edu/drupal/book/export/html/644\n#sp package has specific tool for spatial sampling within polygons. Hooray!\nrandomPointOrigins <- spsample(gbmerge, n=pairNum, type='random')\nrandomPointDestinations <- spsample(gbmerge, n=pairNum, type='random')\n\n#In case you wanna see em\n#plot(gbmerge); points(randomPointDestinations, col='red', pch=3, cex=0.5); points(randomPointOrigins, col='blue', pch=3, cex=0.5)\n\n#convert to lat long in prep for google query\nrandomPointOrigins <- spTransform(randomPointOrigins, CRS(\"+init=epsg:4326\"))\nrandomPointDestinations <- spTransform(randomPointDestinations, CRS(\"+init=epsg:4326\"))\n\n#Use dataframe, single row per origin-destination pair\nrandomPointOrigins <- data.frame(randomPointOrigins)\nrandomPointDestinations <- data.frame(randomPointDestinations)\n\n#Distinguish x and y column names (for later CSV writing)\ncolnames(randomPointOrigins)[colnames(randomPointOrigins)==\"x\"] <- \"origin_x\"\ncolnames(randomPointOrigins)[colnames(randomPointOrigins)==\"y\"] <- \"origin_y\"\ncolnames(randomPointDestinations)[colnames(randomPointDestinations)==\"x\"] <- \"dest_x\"\ncolnames(randomPointDestinations)[colnames(randomPointDestinations)==\"y\"] <- \"dest_y\"\n\n#Final set of origin-destination points\npointSet <- cbind(randomPointOrigins,randomPointDestinations)\n\n#Create results matrix, one row per origin-destination pair\n#Storing four results: distance and time of each route\n#(Distance in metres, time in seconds)\n#And also the strings for the address of origins and destinations\nresults <- matrix(nrow=pairNum , ncol=4)\n\n#Iterate over required pair numbers, get data from google\nfor(i in 1:pairNum) {\n\n#set google distance matrix query\n#Google does y,x. Reverse order\n#See https://developers.google.com/maps/documentation/distancematrix/ for option info\nqry <- paste(\"origins=\", pointSet[i,2] , \",\" , pointSet[i,1] ,\n             \"&destinations=\" ,pointSet[i,4] , \",\" , pointSet[i,3] ,\n             \"&sensor=FALSE\",\n#              \"&mode=bicycling\",\n             \"&avoid=ferries\",#not going to any islands!\n             sep=\"\"#no spaces\n)\n\n#Get the JSON\ngimme <- GET(\n  testURL,  \n  query = qry,\n  #If using in Leeds University, obv: comment this out if not, or if using Leeds Uni wifi\n  #Use this to see details of proxy connection: c(use_proxy(\"www-cache.leeds.ac.uk:3128\", 8080), verbose())\n  c(use_proxy(\"www-cache.leeds.ac.uk:3128\", 8080))\n)\n\n#http://blog.rstudio.org/2014/03/21/httr-0-3/\nstop_for_status(gimme)\n\nstore <- content(gimme)\n\n#if result was OK, keep\n# if(store$status==\"OK\") {\nif(store$rows[[1]]$elements[[1]]$status==\"OK\") {\n  results[i,1] <- store$rows[[1]]$elements[[1]]$distance$value\n  results[i,2] <- store$rows[[1]]$elements[[1]]$duration$value\n  results[i,3] <- store$origin_addresses[[1]]\n  results[i,4] <- store$destination_addresses[[1]]\n} else {\n  fail <- fail + 1\n}\n\n#pause between API calls. We're aiming for:\n# \"100 elements per 10 seconds.\"\n# \"2500 elements per 24 hour period.\"\n#Two elements per call (origin and destination)\n#Being conservative: 0.3 seconds should hit ~66 elements per 10 seconds\nSys.sleep(0.3)\n\nprint(paste(\"API call\", i, \"complete, status: \", store$rows[[1]]$elements[[1]]$status))\n\n}#end for\n\n#Append the coordinates used\nreadyForWriting <- cbind(data.frame(results),pointSet)\n\ncolnames(readyForWriting)[colnames(readyForWriting)==\"X1\"] <- \"distance\"\ncolnames(readyForWriting)[colnames(readyForWriting)==\"X2\"] <- \"time\"\ncolnames(readyForWriting)[colnames(readyForWriting)==\"X3\"] <- \"origin\"\ncolnames(readyForWriting)[colnames(readyForWriting)==\"X4\"] <- \"destination\"\n\n#Strip out failed calls\nreadyForWriting <- readyForWriting[ !is.na(readyForWriting$distance) ,]\n\n#Write the final results file, unique name each time\nfilename = paste(\"GoogleDistanceMatrixRandomPathRresults_\",date(),\".csv\",sep=\"\")\n\n#spaces with underscores\nfilename <- gsub(\" \", \"_\", filename)\n#colons with underscores\nfilename <- gsub(\":\", \"_\", filename)\n\nwrite.csv(readyForWriting, filename)\n\nprint(paste(pairNum, \"attempts, \", (pairNum - fail), \"successful.\"))\nprint(paste(\"File written: \", filename))\n",
    "created" : 1434555129759.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2444150550",
    "id" : "10C6C49B",
    "lastKnownWriteTime" : 1435656974,
    "path" : "C:/Users/geodo/Dropbox/R/Workspace/randomNetworkDistancer/routesRandomiser.R",
    "project_path" : "routesRandomiser.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}